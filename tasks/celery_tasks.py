import os
import shutil
import tempfile
import asyncio
import io
import csv
import uuid
import pandas as pd
from datetime import datetime, timezone, timedelta
from telegram import Bot, InputMediaDocument, InlineKeyboardMarkup, InlineKeyboardButton
from asgiref.sync import async_to_sync

from celery_app import celery
import config
from services import supabase_service, queue_service
from workers.get_trader_pnl import perform_pnl_fetch
from workers.get_program_swaps import perform_program_swaps
from workers.get_top_traders import perform_toplevel_traders_fetch
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import logging

logging.basicConfig(level=logging.INFO) # Basic configuration
logger = logging.getLogger(__name__)

# --- Ð›Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸ ---
TOKENS_CHUNK_SIZE = 1000
TRADERS_CHUNK_SIZE = 40000

def init_worker_driver():
    """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Selenium-Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð°."""
    print("CELERY_TASK: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Selenium-Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð°...")
    opts = Options()
    opts.add_argument(f"--user-data-dir={config.CHROME_PROFILE_PATH}")
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ñ
    prefs = {
        "download.default_directory": config.DOWNLOAD_DIR,
        "download.prompt_for_download": False,
    }
    opts.add_experimental_option("prefs", prefs)
    driver = webdriver.Chrome(options=opts)
    print("CELERY_TASK: Ð”Ñ€Ð°Ð¹Ð²ÐµÑ€ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½.")
    return driver

# --- Ð¤Ð¾Ð½Ð¾Ð²Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Celery ---

@celery.task
async def _pnl_fetch_async(wallets: list, chat_id: int):
    """
    ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ PNL.
    Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð£Ð±Ñ€Ð°Ð½Ñ‹ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¾ Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐµ Ð½Ð° Ð¿Ð°ÐºÐµÑ‚Ñ‹.
    """
    bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
    temp_files_to_clean = []
    
    try:
        unique_wallets = list(set(wallets))
        wallet_chunks = [unique_wallets[i:i + TRADERS_CHUNK_SIZE] for i in range(0, len(unique_wallets), TRADERS_CHUNK_SIZE)]
        all_pnl_files = []

        # # --- Ð‘Ð›ÐžÐš Ð£Ð’Ð•Ð”ÐžÐœÐ›Ð•ÐÐ˜Ð™ Ð£Ð”ÐÐ›Ð•Ð ---

        for i, chunk in enumerate(wallet_chunks, 1):
            driver = None
            try:
                driver = init_worker_driver()
                result_path = perform_pnl_fetch(driver, chunk)
                if result_path:
                    all_pnl_files.append(result_path)
                    temp_files_to_clean.append(result_path)
            finally:
                if driver: driver.quit()

        if not all_pnl_files:
            raise Exception("PNL fetch failed for all chunks.")

        if len(all_pnl_files) > 1:
            print("Merging PNL reports...")
            df_list = [pd.read_csv(path) for path in all_pnl_files]
            merged_df = pd.concat(df_list, ignore_index=True).drop_duplicates(subset=['wallet'])
            final_path = os.path.join(config.FILES_DIR, f"pnl_merged_{uuid.uuid4()}.csv")
            merged_df.to_csv(final_path, index=False)
            temp_files_to_clean.append(final_path)
        else:
            final_path = all_pnl_files[0]
        
        caption = f"âœ… Ð’Ð°Ñˆ PNL-Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð³Ð¾Ñ‚Ð¾Ð². ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(unique_wallets)} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾ÑˆÐµÐ»ÑŒÐºÐ¾Ð²."
        back_button_markup = InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ð² Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]])
        
        with open(final_path, "rb") as f:
            await bot.send_document(chat_id=chat_id, document=f, caption=caption, reply_markup=back_button_markup)

    except Exception as e:
        print(f"CELERY_ERROR: Ð—Ð°Ð´Ð°Ñ‡Ð° PNL Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»Ð°ÑÑŒ: {e}")
        await bot.send_message(chat_id=chat_id, text="âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° PNL.")
    finally:
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)

@celery.task
async def run_swaps_fetch_task(program: str, interval: str, chat_id: int):
    """
    Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Program Swaps.
    """
    print(f"CELERY_TASK: Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð° Program Swaps Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° {chat_id}.")
    driver = None
    bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
    try:
        # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐŸÑ€Ð¸ÑÐ²Ð°Ð¸Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð½Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹
        driver = init_worker_driver()
        
        file_path = perform_program_swaps(driver, program, interval)

        if file_path and os.path.exists(file_path):
            back_button_markup = InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ð² Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]])
            with open(file_path, "rb") as f:
                asyncio.run(bot.send_document(
                    chat_id=chat_id, document=f, caption=f"âœ… Ð’Ð°Ñˆ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ Program Swaps Ð´Ð»Ñ `{program}` Ð³Ð¾Ñ‚Ð¾Ð².", reply_markup=back_button_markup
                ))
            # os.remove(file_path)
        else:
            asyncio.run(bot.send_message(chat_id=chat_id, text=f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ Program Swaps Ð¾Ñ‚ Discord-Ð±Ð¾Ñ‚Ð°."))
    except Exception as e:
        print(f"CELERY_ERROR: Ð—Ð°Ð´Ð°Ñ‡Ð° Swaps Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»Ð°ÑÑŒ: {e}")
        asyncio.run(bot.send_message(chat_id=chat_id, text="âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° Swaps."))
    finally:
        # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð°
        if driver:
            driver.quit()
        print(f"CELERY_TASK: Ð”Ñ€Ð°Ð¹Ð²ÐµÑ€ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Swaps (Ñ‡Ð°Ñ‚ {chat_id}) Ð±Ñ‹Ð» Ð·Ð°ÐºÑ€Ñ‹Ñ‚.")

@celery.task
async def _traders_fetch_async(file_content_str: str, chat_id: int):
    """
    ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð².
    Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð£Ð±Ñ€Ð°Ð½Ñ‹ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¾ Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐµ Ð½Ð° Ð¿Ð°ÐºÐµÑ‚Ñ‹.
    """
    bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
    temp_files_to_clean = []
    
    try:
        token_addresses = [line.strip() for line in file_content_str.splitlines() if line.strip()]
        token_chunks = [token_addresses[i:i + TOKENS_CHUNK_SIZE] for i in range(0, len(token_addresses), TOKENS_CHUNK_SIZE)]
        all_traders_files = []

        # # --- Ð‘Ð›ÐžÐš Ð£Ð’Ð•Ð”ÐžÐœÐ›Ð•ÐÐ˜Ð™ Ð£Ð”ÐÐ›Ð•Ð ---

        for i, chunk in enumerate(token_chunks, 1):
            driver = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix=".txt", encoding='utf-8') as tmp_f:
                    tmp_f.write("\n".join(chunk))
                    temp_filepath = tmp_f.name
                    temp_files_to_clean.append(temp_filepath)
                
                driver = init_worker_driver()
                result_path = perform_toplevel_traders_fetch(driver, temp_filepath)
                if result_path:
                    all_traders_files.append(result_path)
            finally:
                if driver: driver.quit()

        if not all_traders_files:
            raise Exception("Top Traders fetch failed for all chunks.")
        
        final_trader_list = []
        for path in all_traders_files:
            with open(path, 'r', encoding='utf-8') as f:
                final_trader_list.extend([line.strip() for line in f if line.strip() and not line.startswith('---')])
            temp_files_to_clean.append(path)
        
        final_text = "\n".join(final_trader_list)
        final_path = os.path.join(config.TOP_TRADERS_DIR, f"traders_merged_{uuid.uuid4()}.txt")
        temp_files_to_clean.append(final_path)
        with open(final_path, 'w', encoding='utf-8') as f: f.write(final_text)

        caption = f"âœ… Ð’Ð°Ñˆ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ Ñ‚Ð¾Ð¿-Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð°Ð¼ Ð³Ð¾Ñ‚Ð¾Ð². ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(token_addresses)} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²."
        back_button_markup = InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ð² Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]])
        
        with open(final_path, "rb") as f:
            await bot.send_document(chat_id=chat_id, document=f, caption=caption, filename="top_traders_merged.txt", reply_markup=back_button_markup)

    except Exception as e:
        print(f"CELERY_ERROR: Ð—Ð°Ð´Ð°Ñ‡Ð° Top Traders Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»Ð°ÑÑŒ: {e}")
        await bot.send_message(chat_id=chat_id, text="âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.")
    finally:
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)
        
@celery.task
def run_token_parse_task(chat_id: int, settings: dict):
    """
    Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¿Ð¾ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼.
    Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐ: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ async_to_sync Ð´Ð»Ñ Ð²Ñ‹Ð·Ð¾Ð²Ð° async-Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹.
    """
    print(f"CELERY_TASK: Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð° Token Parse Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° {chat_id}")
    bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð±ÐµÑ€Ñ‚ÐºÑƒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÐ¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ Ð½Ð°ÑˆÐ¸ async-Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
    sync_send_message = async_to_sync(bot.send_message)
    sync_send_document = async_to_sync(bot.send_document)
    
    try:
        platforms = settings.get('platforms', [])
        period_key = settings.get('period', '24h')
        categories = settings.get('categories', [])
        lang = settings.get('lang', 'en')
        
        hours = int(period_key.replace('h', ''))
        start_time = datetime.now(timezone.utc) - timedelta(hours=hours)

        # Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ ÑÐµÑ€Ð²Ð¸Ñ
        results = async_to_sync(supabase_service.fetch_tokens_by_criteria)(start_time, platforms, categories)
        
        if not results:
            sync_send_message(chat_id=chat_id, text="ðŸ¤· ÐŸÐ¾ Ð²Ð°ÑˆÐ¸Ð¼ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.")
            return

        df = pd.DataFrame(results)
        output = io.StringIO()
        fieldnames = ["contract_address", "ticker", "name", "migration_time", "launchpad", "category"]
        df_final = df.reindex(columns=fieldnames)
        df_final.to_csv(output, index=False, header=True)
        
        csv_file_bytes = io.BytesIO(output.getvalue().encode('utf-8'))
        csv_file_bytes.name = f"tokens_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        caption = f"âœ… Ð’Ð°Ñˆ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð³Ð¾Ñ‚Ð¾Ð². ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(df)} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²."
        back_button_markup = InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ð² Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]])

        sync_send_document(
            chat_id=chat_id, 
            document=csv_file_bytes, 
            caption=caption,
            reply_markup=back_button_markup
        )
        
        print(f"CELERY_TASK: Ð—Ð°Ð´Ð°Ñ‡Ð° Token Parse Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° {chat_id} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°.")

    except Exception as e:
        print(f"CELERY_ERROR: Ð—Ð°Ð´Ð°Ñ‡Ð° Token Parse Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»Ð°ÑÑŒ: {e}")
        sync_send_message(chat_id=chat_id, text="âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð².")
        
@celery.task
async def _all_in_parse_pipeline_async(chat_id: int, template: dict, message_id: int):
    """
    ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð°.
    Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐŸÑ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚ message_id Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ.
    """
    bot = Bot(token=config.TELEGRAM_BOT_TOKEN)
    temp_files_to_clean = []
    
    try:
        # --- Ð£Ð²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ Ð¾ ÑÑ‚Ð°Ñ€Ñ‚Ðµ ---
        # Ð­Ñ‚Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð² Ñ‡Ð°Ñ‚Ðµ
        start_text = (
            f"âœ… Ð’Ð°ÑˆÐ° Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ Ð¿Ð¾Ð´Ð¾ÑˆÐ»Ð°! ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ 'All-In Parse' Ð¿Ð¾ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñƒ '{template.get('template_name', '...')}'.\n\n"
            "Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð¼Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. Ð¯ Ð±ÑƒÐ´Ñƒ Ð¿Ñ€Ð¸ÑÑ‹Ð»Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¿Ð¾ Ð¼ÐµÑ€Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸."
        )
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=start_text)

        # --- Ð­Ð¢ÐÐŸ 1: GET TOKENS ---
        # Ð£Ð²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾Ð± ÑÑ‚Ð°Ð¿Ð°Ñ… Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ‚Ð¾Ð¶Ðµ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ ÐºÐ°Ðº Ð½Ð¾Ð²Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸš€ Ð­Ñ‚Ð°Ð¿ 1/3: ÐŸÐ¾Ð¸ÑÐº Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²...")
        hours = int(template.get('time_period', '24h').replace('h', ''))
        start_time = datetime.now(timezone.utc) - timedelta(hours=hours)
        categories = [cat for cat in template.get('categories', []) if cat in ['completed', 'completing']]
        tokens = await supabase_service.fetch_tokens_by_criteria(start_time, template.get('platforms', []), categories)

        if not tokens:
            await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñƒ. Ð—Ð°Ð´Ð°Ñ‡Ð° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°.")
            return

        token_addresses = [t['contract_address'] for t in tokens]
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"âœ… Ð­Ñ‚Ð°Ð¿ 1 Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½. ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(tokens)} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð².")

        # --- Ð­Ð¢ÐÐŸ 2: GET TOP TRADERS ---
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸ‘¥ Ð­Ñ‚Ð°Ð¿ 2/3: ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð² Ð´Ð»Ñ {len(token_addresses)} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð². Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼Ñ...")
        
        token_chunks = [token_addresses[i:i + TOKENS_CHUNK_SIZE] for i in range(0, len(token_addresses), TOKENS_CHUNK_SIZE)]
        all_traders_files = []
        
        for i, chunk in enumerate(token_chunks, 1):
            if len(token_chunks) > 1:
                await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸ‘¥ Ð­Ñ‚Ð°Ð¿ 2/3: ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð¿Ð°ÐºÐµÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² {i} Ð¸Ð· {len(token_chunks)}...")
            
            driver_traders = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix=".txt", encoding='utf-8') as tmp_f:
                    tmp_f.write("\n".join(chunk))
                    temp_filepath = tmp_f.name
                    temp_files_to_clean.append(temp_filepath)
                
                driver_traders = init_worker_driver()
                result_path = perform_toplevel_traders_fetch(driver_traders, temp_filepath)
                if result_path:
                    all_traders_files.append(result_path)
            finally:
                if driver_traders: driver_traders.quit()

        if not all_traders_files:
             await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð². Ð—Ð°Ð´Ð°Ñ‡Ð° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°.")
             raise Exception("Top Traders fetch failed")

        final_trader_list = []
        for path in all_traders_files:
            with open(path, 'r', encoding='utf-8') as f:
                final_trader_list.extend([line.strip() for line in f if line.strip() and not line.startswith('---')])
            temp_files_to_clean.append(path)
        
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="âœ… Ð­Ñ‚Ð°Ð¿ 2 Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½. Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð² ÑÐ¾Ð±Ñ€Ð°Ð½.")

        # --- Ð­Ð¢ÐÐŸ 3: GET PNL ---
        unique_traders = list(set(final_trader_list))
        if not unique_traders:
            await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="âš ï¸ Ð¢Ñ€ÐµÐ¹Ð´ÐµÑ€Ñ‹ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° PNL Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹. Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ.")
            return

        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸ“Š Ð­Ñ‚Ð°Ð¿ 3/3: ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ PNL Ð´Ð»Ñ {len(unique_traders)} Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð².")
        
        trader_chunks = [unique_traders[i:i + TRADERS_CHUNK_SIZE] for i in range(0, len(unique_traders), TRADERS_CHUNK_SIZE)]
        all_pnl_reports_paths = []
        
        for i, chunk in enumerate(trader_chunks, 1):
            if len(trader_chunks) > 1:
                await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸª“ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ PNL-Ð¿Ð°ÐºÐµÑ‚ {i} Ð¸Ð· {len(trader_chunks)}...")
            
            driver_pnl = None
            try:
                driver_pnl = init_worker_driver()
                pnl_csv_path = perform_pnl_fetch(driver_pnl, chunk)
                if pnl_csv_path:
                    all_pnl_reports_paths.append(pnl_csv_path)
                    temp_files_to_clean.append(pnl_csv_path)
            finally:
                if driver_pnl: driver_pnl.quit()

        if not all_pnl_reports_paths:
            await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ PNL Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°. Ð—Ð°Ð´Ð°Ñ‡Ð° Ð¿Ñ€ÐµÑ€Ð²Ð°Ð½Ð°.")
            raise Exception("PNL fetch failed")

        # --- Ð­Ð¢ÐÐŸ 4: ÐžÐ‘ÐªÐ•Ð”Ð˜ÐÐ•ÐÐ˜Ð• Ð˜ ÐžÐ¢ÐŸÐ ÐÐ’ÐšÐ PNL ---
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=f"ðŸ–‡ï¸ Ð­Ñ‚Ð°Ð¿ 4/4: ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ PNL-Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²...")
        
        df_list = [pd.read_csv(path) for path in all_pnl_reports_paths]
        merged_df = pd.concat(df_list, ignore_index=True).drop_duplicates(subset=['wallet'])
        
        # --- ÐÐžÐ’Ð«Ð™ Ð¨ÐÐ“: ÐŸÐ Ð˜ÐœÐ•ÐÐ¯Ð•Ðœ ÐŸÐ ÐžÐ”Ð’Ð˜ÐÐ£Ð¢Ð«Ð• Ð¤Ð˜Ð›Ð¬Ð¢Ð Ð« ---
        pnl_filters = template.get('pnl_filters', {})
        if pnl_filters:
            logger.info(f"Applying PNL filters: {pnl_filters}")
            filtered_df = apply_pnl_filters(merged_df, pnl_filters)
        else:
            filtered_df = merged_df # Ð•ÑÐ»Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² Ð½ÐµÑ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ð¹ DF
        # ----------------------------------------------------

        final_filename = f"all_in_parse_final_pnl_{uuid.uuid4()}.csv"
        final_csv_path = os.path.join(config.FILES_DIR, final_filename)
        filtered_df.to_csv(final_csv_path, index=False) # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ DF
        temp_files_to_clean.append(final_csv_path)

        caption = (
            f"âœ… All-In Parse Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½!\n\n"
            f"ÐÐ½Ð°Ð»Ð¸Ð· Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ:\n"
            f"  - Ð¢Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: {len(tokens)}\n"
            f"  - Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð²: {len(unique_traders)}\n\n"
            f"Ð’ ÑÑ‚Ð¾Ð¼ Ñ„Ð°Ð¹Ð»Ðµ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ PNL-Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð´Ð»Ñ {len(filtered_df)} Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€Ð¾Ð² (Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸)."
        )
        
        back_button_markup = InlineKeyboardMarkup([[InlineKeyboardButton("â¬…ï¸ ÐÐ°Ð·Ð°Ð´ Ð² Ð¼ÐµÐ½ÑŽ", callback_data="main_menu")]])
        with open(final_csv_path, "rb") as f:
            await bot.send_document(
                chat_id=chat_id, document=f,
                caption=caption,
                reply_markup=back_button_markup
            )
        
        await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text="Ð’ÑÐµ Ð³Ð¾Ñ‚Ð¾Ð²Ð¾!")
        
    except Exception as e:
        print(f"CELERY_ERROR: 'All-In Parse' Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»ÑÑ: {e}")
        error_text = f"âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ 'All-In Parse'."
        try:
            await bot.edit_message_text(chat_id=chat_id, message_id=message_id, text=error_text)
        except:
            await bot.send_message(chat_id=chat_id, text=error_text)
    finally:
        for f_path in temp_files_to_clean:
            if os.path.exists(f_path):
                os.remove(f_path)
        print("CELERY_TASK: 'All-In Parse' Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹.")
        
def apply_pnl_filters(df: pd.DataFrame, filters: dict) -> pd.DataFrame:
    """ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ PNL-Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹ Ðº DataFrame."""
    if not filters:
        return df

    filtered_df = df.copy()
    for column, rules in filters.items():
        if column not in filtered_df.columns:
            continue
        
        # Ð£Ð±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð°Ñ, Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÑ Ð¾ÑˆÐ¸Ð±ÐºÐ¸
        filtered_df[column] = pd.to_numeric(filtered_df[column], errors='coerce')
        
        min_val = rules.get('min')
        max_val = rules.get('max')

        if min_val is not None:
            filtered_df = filtered_df[filtered_df[column] >= min_val]
        if max_val is not None:
            filtered_df = filtered_df[filtered_df[column] <= max_val]
            
    return filtered_df

@celery.task
def run_all_in_parse_pipeline_task_wrapper(chat_id: int, template: dict, message_id: int):
    """
    Ð¡Ð˜ÐÐ¥Ð ÐžÐÐÐÐ¯ Ð·Ð°Ð´Ð°Ñ‡Ð°-Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð½Ð°Ñˆ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½.
    """
    async_to_sync(_all_in_parse_pipeline_async)(chat_id, template, message_id) # <-- Ð²ÐµÑ€Ð½ÑƒÐ»Ð¸ message_id
    
@celery.task
def run_pnl_fetch_task(wallets: list, chat_id: int):
    """Ð¡Ð˜ÐÐ¥Ð ÐžÐÐÐÐ¯ Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ° Ð´Ð»Ñ _pnl_fetch_async."""
    async_to_sync(_pnl_fetch_async)(wallets, chat_id)

@celery.task
def run_traders_fetch_task(file_content_str: str, chat_id: int):
    """Ð¡Ð˜ÐÐ¥Ð ÐžÐÐÐÐ¯ Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ° Ð´Ð»Ñ _traders_fetch_async."""
    async_to_sync(_traders_fetch_async)(file_content_str, chat_id)